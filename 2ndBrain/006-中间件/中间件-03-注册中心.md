# 中间件-03-注册中心


[TOC]


## 更新
* 2022/06/06，撰写

## 参考资料
* [如何设计一个注册中心](https://mp.weixin.qq.com/s/BhIbcwQYEu1M-OHVbBYS_Q)
* [4种微服务注册中心如何选型 | 掘金](https://juejin.cn/post/7012084821224603656)
* [注册中心选型篇](https://jishuin.proginn.com/p/763bfbd29957)




## 注册中心高频面试题

* [12道 Zookeeper 高频面试题，你顶得住吗](https://www.developers.pub/article/395)


### ZK可以做什么

Zookeeper 是一个开源的，是用于维护配置信息，命名，提供分布式同步和提供组服务的集中式服务。

可以基于 Zookeeper 实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能。



## 如何设计注册中心

如何设计一个注册中心？需要考虑下面的问题

1. 服务如何注册
2. consumer 如何知道 provider
3. 服务注册中心如何高可用（集群部署）
4. 服务上下线，消费端如何动态感知
   * 发布-订阅模式（经典案例是 Zookeeper）
   * 主动拉取策略（如心跳检测，经典案例是 Eureka）

> 心跳检测（`heartBeat`）指的是服务注册中心每隔一定时间去监测一下 `provider`，如果监测到某个服务挂了，那就把对应服务地址从服务列表中删除。


## 主流的注册中心产品

目前主流的注册中心产品有以下 4 种
1. Zookeeper
2. Eureka
3. Consul
4. Kubernetes




| 比较项	 | Eureka  | zookeeper | nacos | consul  |
|--------|---------|-----------|-------|---------|
| 集群结构 | 平级 | 	主从	| 支持平级和主从	| 主从 |
| 集群角色 | 主人	| leader、follower、observer | leader、follower、candidate |	server-leader、server 以及 client |
| 是否可以及时知道服务状态变化 | 不可以 | 可以 | 不可以 | 不可以 |
| 一致性协议（CAP） | 注重可用性（AP）| 注重一致性(CP) | 支持CP和AP | 注重一致性(CP)|
| 雪崩保护	| 有	| 没有 |	有	 | 没有 |
| 社区活跃度 |	Eureka2.0 不再维护了 | 	持续维护	| 持续维护	| 持续维护 |
| 管理端	| 有现成的eureka管理端	| 没有现成的管理端 | 有现成的管理端 | 有现成的管理端 |
| 负载均衡策略	| 使用ribbon实现 | 一般可以直接采用RPC的负载均衡 |	权重/metadata/selector | fabio  |
| 权限控制 |无	| 使用ACL实现节点权限控制 | RBAC-用户、角色、权限	| ACL | Spring Cloud集成 | 支持	| 支持	| 支持	| 支持 |
| 健康检查 | Client Beat | Keep Alive | TCP/HTTP/MYSQL/Client | Beat	| TCP/HTTP/gRPC/Cmd |
| 自动注销实例	| 支持 | 支持	| 支持	| 不支持 |
| 监听支持	| 支持 | 支持	| 支持	| 支持 |
| 访问协议 | HTTP	 | TCP |	HTTP/DNS	| HTTP/DNS |
| 是否可用作配置中心	| 否	| 是	| 是	| 是 |
| 多数据中心	| 不支持	| 不支持	| 不支持	| 支持 |
| 跨注册中心同步	| 不支持	| 不支持	| 支持	| 支持 | 
| Dubbo集成	| 不支持	| 支持	| 支持	| 不支持 |
| K8S集成	| 支持 | 支持	| 支持	| 支持 |


## Zookeeper




### 3 种角色

|    角色     |          说明            |
|------------|--------------------------|
| Leader 角色 | **一个 Zookeeper 集群同一时间只会有一个实际工作的 Leader**，它会发起并维护与各 Follwer 及 Observer 间的心跳，所有的写操作必须要通过 Leader 完成再由 Leader 将写操作广播给其它服务器 |
| Follower角色 | 一个 Zookeeper 集群可能同时存在多个 Follower，它会响应Leader 的心跳，Follower 可直接处理并返回客户端的读请求，**同时会将写请求转发给 Leader 处理**，并且负责在 Leader 处理写请求时对请求进行投票 |
| Observer角色 | 与 Follower 类似，但是无投票权 |


### 4 种节点

|    节点   |         说明          |
|----------|-----------------------|
| PERSISTENT-持久节点 | 除非手动删除，否则节点一直存在于 Zookeeper 上 |
| EPHEMERAL-临时节点 | 临时节点的生命周期与客户端会话绑定，一旦客户端会话失效，那么这个客户端创建的所有临时节点都会被移除 |
| PERSISTENT_SEQUENTIAL-持久顺序节点 | 基本特性同持久节点，只是增加了顺序属性，节点名后边会追加一个由父节点维护的自增整型数字 | 
| EPHEMERAL_SEQUENTIAL-临时顺序节点 | 基本特性同临时节点，增加了顺序属性，节点名后边会追加一个由父节点维护的自增整型数字 |


> 顺序节点（sequential node），每次创建顺序节点时，ZooKeeper 都会在路径后面自动添加上 10 位的数字，从 1 开始，最大是 2147483647（2^32-1）。


### watch机制

|     机制     |                说明         |
|-------------|-----------------------------|
| watch 机制 | 是一个轻量级的设计，采用了一种推拉结合的模式。一旦服务端感知主题变了，那么只会发送一个事件类型和节点信息给关注的客户端，而不会包括具体的变更内容，所以事件本身是轻量级的，这就是推的部分。然后，收到变更通知的客户端需要自己去拉变更的数据，这就是拉的部分。 | 


client 会对某个 znode 注册一个 watcher 事件，当该 znode 发生变化时，这些 client 会收到 ZooKeeper 的通知。

watch 机制有下面 4 个特性
1. 一次性
   * 一旦一个 Watcher 触发之后，Zookeeper 就会将它从存储中移除，如果还要继续监听这个节点，就需要我们在客户端的监听回调中，再次对节点的监听 watch 事件设置为 True。否则客户端只能接收到一次该节点的变更通知
2. 客户端串行
   * 客户端的 Wather 回调处理是串行同步的过程，不要因为一个 Wather 的逻辑阻塞整个客户端
3. 轻量
   * Wather 通知的单位是 WathedEvent，只包含通知状态、事件类型和节点路径，不包含具体的事件内容，具体的时间内容需要客户端主动去重新获取数据
4. 异步
   * Zookeeper 服务器发送 watcher 的通知事件到客户端是异步的，不能期望能够监控到节点每次的变化
   * Zookeeper 只能保证最终的一致性，而无法保证强一致性



### ZXID


ZooKeeper 采用全局递增的事务 Id 来标识，所有 proposal（建议）在被提出的时候加上了 ZooKeeper Transaction Id ，zxid 是 64 位的 Long 类型，这是保证事务的顺序一致性的关键。zxid 中高 32 位表示纪元 epoch，低 32 位表示事务标识xid。你可以认为 zxid 越大说明存储数据越新。

每个 leader 都会具有不同的 epoch 值，表示一个纪元/朝代，用来标识 leader 周期。每个新的选举开启时都会生成一个新的 epoch，新的 leader 产生的话 epoch 会自增，会将该值更新到所有的 zkServer 的 zxid 和 epoch。

zxid 是一个依次递增的事务编号。数值越大说明数据越新，所有 proposal（建议）在被提出的时候加上了 zxid，然后会依据数据库的两阶段过程，首先会向其他的 server 发出事务执行请求，如果超过半数的机器都能执行并且能够成功，那么就会开始执行。


### Zookeeper有哪些特性

1. 顺序一致性
   * **leader 会根据请求顺序生成 ZXID 来严格保证请求顺序的下发执行。**
2. 原子性
   * 所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的，要么成功，要么就失败。
3. 单一视图
   * 无论客户端连到哪一个 ZooKeeper 服务器上，看到的数据都是一致的。
4. 可靠性
   * 一旦服务端成功地应用了一个事务，并完成对客户端的响应，那么该事务所引起的服务端状态变更将会被一直保留下来。
5. 实时性
   * Zookeeper 仅仅能保证在一段时间内客户端最终一定能够从服务端上读取到最新的数据状态。


### 数据一致性和原子广播协议（ZAB）

Zookeeper 支持 CAP 理论中的 CP，主要是一致性（`consistency`）。那么，Zookeeper 是怎么保证强一致性呢？

Zookeeper 的强一致性是通过「原子广播协议（ZAB，`Zookeeper Atomic Broadcast`）」实现的。

Zookeeper 使用原子广播协议来保证数据的强一致性，其主要内容包括
1. 对节点进行角色划分，划分出 `leader` 和 `follower`
2. 发送一个 Proposal（提议）后，leader 给所有的 follower 都同步一个 Proposal（提议）。如果半数以上的 follower，都收到事务的 proposal 提议，每个follower 都会返回一个 Ack。
3. leader 如果异常宕机，会从 follower 中重新选举。



### ZAB的两种工作模式

* [讲解 Zookeeper 的五个核心知识点](https://www.51cto.com/article/641025.html)



ZAB (Zookeeper Atomic Broadcast 原子广播协议) 协议是分布式协调服务。

ZooKeeper 专门设计的一种支持崩溃恢复的一致性协议。基于该协议，ZooKeeper 实现了一种主从模式的系统架构来保持集群中各个副本之间的数据一致性。

分布式系统中 leader 负责外部客户端的写请求。follower 服务器负责读跟同步。这时需要解决俩问题。
1. Leader 服务器是如何把数据更新到所有的 Follower 的。
2. Leader 服务器突然间失效了，集群咋办?

因此，ZAB 协议为了解决上面两个问题而设计了两种工作模式，整个 Zookeeper 就是在这两个模式之间切换
1. 原子广播模式：把数据更新到所有的 follower。
2. 崩溃恢复模式：Leader 发生崩溃时，如何恢复。






#### 原子广播模式

你可以认为消息广播机制是简化版的 2PC协议，就是通过如下的机制保证事务的顺序一致性的。

![](https://image-bed-20181207-1257458714.cos.ap-shanghai.myqcloud.com/back-end-2023/zk-zab-broadcast-1.png)


* leader 从客户端收到一个写请求后生成一个新的事务并为这个事务生成一个唯一的ZXID。
* leader 将将带有 zxid 的消息作为一个提案(proposal)分发给所有 FIFO 队列。
* FIFO队列取出队头proposal给follower节点。
* 当 follower 接收到 proposal，先将 proposal 写到硬盘，写硬盘成功后再向 leader 回一个 ACK。
* FIFO队列把 ACK 返回给Leader。
* 当leader收到超过一半以上的follower的ack消息，leader会进行commit请求，然后再给FIFO发送commit请求。
* 当follower收到commit请求时，会判断该事务的ZXID是不是比历史队列中的任何事务的ZXID都小，如果是则提交，如果不是则等待比它更小的事务的commit(保证顺序性)


#### 崩溃恢复

消息广播过程中，Leader 崩溃了还能保证数据一致吗？当 Leader 崩溃会进入崩溃恢复模式。其实主要是对如下两种情况的处理。
1. Leader 在复制数据给所有 Follwer 之后崩溃，咋搞?
2. Leader 在收到 Ack 并提交了自己，同时发送了部分 commit 出去之后崩溃咋办?

针对此问题，ZAB 定义了 2 个原则
1. ZAB 协议确保执行那些已经在 Leader 提交的事务最终会被所有服务器提交。
2. ZAB 协议确保丢弃那些只在 Leader 提出/复制，但没有提交的事务。

至于如何实现确保提交已经被 Leader 提交的事务，同时丢弃已经被跳过的事务呢?关键点就是依赖上面说到过的 ZXID了。


### ZK 中只有一个leader

* ref 1-[Zookeeper 只有一个 leader 节点负责写会不会压力大 | Segmentfault](https://segmentfault.com/q/1010000023295773)


思考下面两个问题

1. zookeeper 只有 leader 节点负责写，会不会压力大?
2. 读的时候怎么读，是每个 follower 节点都可以负责读吗? 怎么保证读的数据是最新提交的? 万一某个 follower 节点不是最新的数据，而被读取了怎么办？


此处，做如下回答。
1. 首先，绝大部分业务中，读要比写的操作多，而且相差的不是一点半点而是几个数量级。说白了就是新增、修改、删除的次数要远远比查询的次数少，所以负责写的节点的压力一定是比负责读的节点压力小。
2. **其次，ZK 其实并不是 Leader 负责写，而是由 Leader 负责通知其他 Follower 写。每个 Follower 也是可以接收写请求的，只不过自己先不执行写，而是要转发给 Leader，等 Leader 通知它写了、才开始写。这一点跟其他一些读写分离方案不太一样，比如 MySql 读写分离的只读节点压根就不接受写请求。**
3. 最后，ZK 写时有半数 ACK 机制，外加 ZAB 协议来确保写入时的强一致性。就读取而言，确实可能读到非最新数据，但其本身是单调读的，再加上提供了 Watch 功能，所以可以确保最终一致。
4. 如果真遇到了读取的不是最终一致性，那只能 Watch 之后再读一次。ZK 保证的是最终一致性，遵循 CAP 定理和 Base 理论，允许中间状态（Base 中的 s）的出现。

> ZK 当初有一些设计最后被证明不太合理，但出于兼容性等历史包袱的考虑，也只能这么着了。可以考虑用用后起之秀 Etcd。



### 重新选举 leader

最后，记录一下「如果 leader 挂掉了，如何重新选举 leader」的过程。
1. 变更状态
   * Leader 挂后，余下的非 Observer 服务器都会将自己的服务器状态变更为 `LOOKING`，然后开始进入 Leader 选举过程。
2. 发起投票
   * 每个非 Observer 的 Server 会发出一个投票（同时会给自己投一票）。和启动过程一致。
3. 接收投票
4. 处理投票
5. 统计投票
6. 改变服务器的状态，设定新的 leader



### 节点数据的最终一致性

ZK 保证的数据节点的最终一致性，而不保证强一致性。

比如两个客户端 A 和 B，分别去读取 node1 和 node2。是有可能出现读取的值不一样的情况的。

如果是比较重要的值，客户端 B 可以调用 `sync()` 方法读取。



### ZK脑裂问题

@todo 此部分笔记待整理


* [脑裂是什么？Zookeeper是如何解决的](https://juejin.cn/post/6844903895387340813)

## Apollo


* [动态线程池调整中 Apollo 的使用](https://blog.csdn.net/riemann_/article/details/116175885)





![](https://image-bed-20181207-1257458714.cos.ap-shanghai.myqcloud.com/back-end-2022/dynamic-config-apollo-1.png)


结合上图，介绍 Apollo 配置中心的使用
1. 客户端和服务端保持了一个长连接，从而能第一时间获得配置更新的推送。（通过 `Http Long Polling` 实现）
2. 客户端还会定时从 Apollo 配置中心服务端拉取应用的最新配置。
3. 客户端从 Apollo 配置中心服务端获取到应用的最新配置后，会保存在内存中。
4. 客户端会把从服务端获取到的配置在本地文件系统缓存一份。
5. 应用程序可以从 Apollo 客户端获取最新的配置、订阅配置更新通知。(客户端既可以主动拉取配置，也可订阅配置中心的通知，接收配置中心 `push` 过来的消息)



### 配置更新推送实现

* [配置更新推送实现 | Apollo 文档](https://github.com/apolloconfig/apollo/wiki/Apollo%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E4%BB%8B%E7%BB%8D)

客户端和服务端保持了一个长连接，从而能第一时间获得配置更新的推送。

长连接实际上我们是通过 Http Long Polling 实现的，具体而言
1. 客户端发起一个 Http 请求到服务端
2. 服务端会保持住这个连接 60 秒
3. 如果在 60 秒内有客户端关心的配置变化，被保持住的客户端请求会立即返回，并告知客户端有配置变化的 namespace 信息，客户端会据此拉取对应 namespace 的最新配置
4. 如果在 60 秒内没有客户端关心的配置变化，那么会返回 Http 状态码 304 给客户端
5. 客户端在收到服务端请求后会立即重新发起连接，回到第一步


> HTTP 304 状态码对应 `Not Modified`，表示的意思是「请求资源与本地缓存相同，未修改」。


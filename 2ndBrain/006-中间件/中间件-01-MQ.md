
# 中间件-01-MQ


[TOC]


## 更新
* 2022/05/17，撰写

## 参考资料
* [MQ消息队列和主流MQ产品对比 | 掘金](https://juejin.cn/post/6896744901665521677)
* [如何回答消息队列的丢失、重复与积压问题 | 架构面试精讲](https://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E7%B2%BE%E8%AE%B2/08%20%20MQ%EF%BC%9A%E5%A6%82%E4%BD%95%E5%9B%9E%E7%AD%94%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%9A%84%E4%B8%A2%E5%A4%B1%E3%80%81%E9%87%8D%E5%A4%8D%E4%B8%8E%E7%A7%AF%E5%8E%8B%E9%97%AE%E9%A2%98.md)
* [如何保证消息不丢失、处理重复消息、消息有序性、消息堆积处理 | 知乎](https://zhuanlan.zhihu.com/p/160243220)
* [MQ 知识梳理 | 面经](https://heapdump.cn/article/3801373)


## MQ高频面试题
* ref 1-[MQ 知识梳理 | 面经](https://heapdump.cn/article/3801373)
* ref 2-[消息队列经典十连问](https://mp.weixin.qq.com/s?__biz=Mzg3NzU5NTIwNg==&mid=2247497847&idx=1&sn=29a32672b712e7dfadfa36c9902b2ec7&chksm=cf22275ef855ae484fb3f51a5726e9a4bc45222e8fbbd33631d177dc4b5619c36889ea178463&token=1077989845&lang=zh_CN#rd)



### 同步消息和异步消息

* ref 1-[同步消息和异步消息 | CSDN](https://blog.csdn.net/qq_41688840/article/details/108020700)


同步消息和异步消息的对比如下图所示。异步消息发送者没有返回值，需要使用 `SendCallback` 接收异步返回结果的回调。

![](https://image-bed-20181207-1257458714.cos.ap-shanghai.myqcloud.com/back-end-2023/mq-async-sync-1.png)



### 如何设计一个消息队列

1. 消息队列的整体流程
   * 整体流程为 producer 发送消息给 broker，broker 存储好，broker 再发送给 consumer 消费，consumer 回复消费确认等
2. RPC 如何设计
   * producer 发送消息给broker，broker 发消息给 consumer 消费，那就需要两次 RPC了
   * RPC 如何设计呢？可以参考开源框架 Dubbo，你可以说说服务发现、序列化协议等等
3. broker 考虑如何持久化
   * 放文件系统还是数据库呢，会不会消息堆积呢，消息堆积如何处理呢
4. 消费关系如何保存
   * 点对点还是广播方式呢？广播关系又是如何维护呢？zk 还是 config server
5. 消费可靠性如何保证
6. 消息队列的高可用
7. 消息事务特性
8. MQ 的伸缩性和可扩展性
   * MQ 得支持可伸缩性吧，就是需要的时候快速扩容，就可以增加吞吐量和容量，那怎么搞？
   * **设计个分布式的系统呗，参照一下 kafka 的设计理念，`broker` -> `topic` -> `partition`，每个 partition 放一个机器，就存一部分数据。**
   * **如果现在资源不够了，简单啊，给 topic 增加 partition，然后做数据迁移，增加机器，不就可以存放更多数据，提供更高的吞吐量了。**



### 说一下RocketMQ的整体工作流程

* ref 1-[MQ 知识梳理 | 面经](https://heapdump.cn/article/3801373)

**简单来说，RocketMQ 是一个分布式消息队列，也就是「消息队列 + 分布式系统」。**
1. **作为消息队列，它是「发 - 存 - 收」的一个模型，对应的就是Producer、Broker、Cosumer；**
2. **作为分布式系统，它要有服务端、客户端、注册中心，对应的就是Broker、Producer/Consumer、NameServer。**


我们看一下它主要的工作流程 RocketMQ 由 NameServer 注册中心集群、Producer 生产者集群、Consumer 消费者集群和若干 Broker（RocketMQ进程）组成。
1. Broker 在启动的时候去向所有的 NameServer 注册，并保持长连接，每 30s 发送一次心跳。
2. Producer 在发送消息的时候，从 NameServer 获取 Broker 服务器地址，根据负载均衡算法选择一台服务器来发送消息。
3. Consumer 消费消息的时候，同样从 NameServer 获取 Broker地址，然后主动拉取消息来消费。

![](https://image-bed-20181207-1257458714.cos.ap-shanghai.myqcloud.com/back-end-2023/rocket-mq-server-step-1.png)



### 为什么RocketMQ不使用Zookeeper作为注册中心

Kafka 我们都知道采用 Zookeeper 作为注册中心，当然也开始逐渐去 Zookeeper。RocketMQ 不使用 Zookeeper，其实主要可能从这几方面来考虑

1. 基于可用性的考虑，根据 CAP 理论，同时最多只能满足两个点，而 Zookeeper 满足的是 CP，也就是说 Zookeeper 并不能保证服务的可用性，Zookeeper 在进行选举的时候，整个选举的时间太长，期间整个集群都处于不可用的状态，而这对于一个注册中心来说肯定是不能接受的，作为服务发现来说就应该是为可用性而设计。
2. 基于性能的考虑，NameServer 本身的实现非常轻量，而且可以通过增加机器的方式水平扩展，增加集群的抗压能力，而 Zookeeper 的写是不可扩展的，Zookeeper 要解决这个问题只能通过划分领域，划分多个 Zookeeper 集群来解决。首先操作起来太复杂，其次这样还是又违反了 CAP 中的 A 的设计，导致服务之间是不连通的。
3. 持久化的机制来带的问题，ZooKeeper 的 ZAB 协议对每一个写请求，会在每个 ZooKeeper 节点上保持写一个事务日志，同时再加上定期的将内存数据镜像（Snapshot）到磁盘来保证数据的一致性和持久性，而对于一个简单的服务发现的场景来说，这其实没有太大的必要，这个实现方案太重了。而且本身存储的数据应该是高度定制化的。
4. 消息发送应该弱依赖注册中心，而 RocketMQ 的设计理念也正是基于此，生产者在第一次发送消息的时候从 NameServer 获取到 Broker 地址后缓存到本地，如果 NameServer 整个集群不可用，短时间内对于生产者和消费者并不会产生太大影响。

### Broker是怎么保存数据的

RocketMQ 主要的存储文件包括
1. CommitLog 文件
2. ConsumeQueue 文件
3. Indexfile 文件


![](https://image-bed-20181207-1257458714.cos.ap-shanghai.myqcloud.com/back-end-2023/rocket-mq-broker-save-1.png)


**<font color='red'>只要消息被刷盘持久化至磁盘文件CommitLog中，那么 Producer 发送的消息就不会丢失。</font>**




>  CommitLog 文件

CommitLog 文件是消息主体以及元数据的存储主体，存储 Producer 端写入的消息主体内容，消息内容不是定长的。单个文件大小默认 1G, 文件名长度为 20 位，左边补零，剩余为起始偏移量，比如 `00000000000000000000` 代表了第一个文件，起始偏移量为 0，文件大小为 1G = 1073741824；当第一个文件写满了，第二个文件为 `00000000001073741824`，起始偏移量为 1073741824，以此类推。消息主要是顺序写入日志文件，当文件满了，写入下一个文件。

CommitLog 文件保存于 `${Rocket_Home}/store/commitlog` 目录中，每个文件默认 1G，写满后自动生成一个新的文件。


> ConsumeQueue 文件

ConsumeQueue：消息消费队列，引入的目的主要是提高消息消费的性能，由于 RocketMQ 是基于主题 topic 的订阅模式，消息消费是针对主题进行的，如果要遍历 commitlog 文件中根据 topic 检索消息是非常低效的。


Consumer 可根据 ConsumeQueue 来查找待消费的消息。其中，ConsumeQueue（逻辑消费队列）作为消费消息的索引，保存了指定 Topic 下的队列消息在 CommitLog 中的起始物理偏移量 offset，消息大小 size 和消息 Tag 的 HashCode 值。


ConsumeQueue 文件可以看成是基于 Topic 的 CommitLog 索引文件，故 ConsumeQueue 文件夹的组织方式为 `topic/queue/file` 三层组织结构，具体存储路径为 `$HOME/store/consumequeue/{topic}/{queueId}/{fileName}`。

同样，ConsumeQueue 文件采取定长设计，每一个条目共 20 个字节，分别为 8 字节的 CommitLog 物理偏移量、4 字节的消息长度、8 字节 tag hashcode，单个文件由 30W 个条目组成，可以像数组一样随机访问每一个条目，每个 ConsumeQueue 文件大小约 5.72M。


> IndexFile 文件

IndexFile：IndexFile（索引文件）提供了一种可以通过 key 或时间区间来查询消息的方法。

Index 文件的存储位置是 `{fileName}`，文件名 `fileName` 是以创建时的时间戳命名的，固定的单个 IndexFile 文件大小约为 400M，一个 IndexFile 可以保存 2000W 个索引，IndexFile 的底层存储设计为在文件系统中实现 HashMap 结构，故 RocketMQ 的索引文件其底层实现为 hash 索引。




## MQ的3大功能




1. 异步处理
2. 应用解耦
3. 流量削峰


![](https://image-bed-20181207-1257458714.cos.ap-shanghai.myqcloud.com/back-end-2022/mq-3-functions-1.png)



## 消息队列模型

消息队列有两种模型
1. 队列模型（集群消费）
2. 发布/订阅模型（广播消费）

### 队列模型

队列模型中，一条消息只能被一个消费者消费。

![](https://image-bed-20181207-1257458714.cos.ap-shanghai.myqcloud.com/back-end-2022/mq-model-1-queue.jpeg)
### 发布/订阅模型

发布/订阅模型中，一条消息能被多个消费者消费。

![](https://image-bed-20181207-1257458714.cos.ap-shanghai.myqcloud.com/back-end-2022/mq-model-2-sub-pub.jpeg)

## MQ中间件产品

目前主流的 MQ 中间件产品包括
1. RabbitMQ
   * 采用 Erlang 语言开发（一种面向并发的编程语言）。
   * 对消息堆积的支持不算太好，当大量消息积压的时候，会导致 RabbitMQ 性能下降。
   * 每秒钟可以处理几万到几十万条消息。

2. RocketMQ
    * 阿里系下开源的一款分布式、队列模型的消息中间件产品。
    * 采用 Java 开发，便于定制化扩展。
    * 面向互联网集群化功能丰富，对在线业务的响应时延做了很多的优化。

3. kafka
    * 采用 Scala 开发。
    * 面向日志功能丰富。

4. ActiveMQ
    * 采用 Java 开发，简单、稳定，但性能一般。


RabbitMQ、RocketMQ、kafka、ActiveMQ，这几种 MQ 中间件产品的对比如下表。


|    对比项    |    RabbitMQ   |    RocketMQ    |    Kafka    |    ActiveMQ  |
|-------------|---------------|----------------|-------------|--------------|
|      协议    |     AMQP      |      AMQP      |   自行设计   |     AMQP     |
|     开发语言  |     Erlang    |      Java      |    Scala   |     Java     |
|     跨语言   |      支持      |       支持      |     支持    |     支持      |
|    单机吞吐量 |      万级      |      十万级     |    百万级   |      万级     |
|   **消息事务**  |   **支持**   |     **支持**  |  **支持**  |    **支持** |
|   可用性     |     高（主从）  |  非常高（分布式） | 非常高（分布式） |  高（主从） |
| 所属社区/公司 | Mozilla Public License |   阿里巴巴，后捐赠给 Apache  |   Apache  |   Apache   |
| 优点 | 跨平台，功能完备，高扩展性 | 功能完备，高扩展性 | 面向日志功能丰富，支持消息大量堆积 | MQ 功能完备，高扩展性 |
|  缺点 | Elang语言难度大，研发人员较少 | 目前只支持 Java 及 C++ | 严格的顺序机制，不支持消息优先级，不支持标准协议 |项目比较陈旧，官方社区在 5.X 之后对其维护越来越少 |
| 综合评价 | 适用于稳定性要求优先的企业级应用 | 阿里系下开源的一款分布式、队列模型的消息中间件产品，国内互联网公司使用居多 | 在日志和大数据方向使用较多 | 小型系统比较适用，但是因为维护越来越少，不建议使用 | 



RabbitMQ
1. 优点：轻量，迅捷，容易部署和使用，拥有灵活的路由配置
2. 缺点：性能和吞吐量不太理想，不易进行二次开发（基于 Erlang 开发）

RocketMQ
1. 优点：性能好，高吞吐量，稳定可靠，有活跃的中文社区
2. 缺点：兼容性上不是太好，支持的客户端语言不多，目前是 Java 及 C++，其中 C++ 不成熟

Kafka
1. 优点：拥有强大的性能及吞吐量，兼容性很好
2. 缺点：由于 “攒一波再处理” 导致延迟比较高




### RocketMQ

* ref 1-[rocketmq.apache.org](https://rocketmq.apache.org/)
* ref 2-[Apache RocketMQ开发者指南 | github](https://github.com/apache/rocketmq/tree/master/docs/cn)


1. RocketMQ 是阿里系下开源的一款分布式、队列模型的消息中间件，是阿里参照 kafka 设计思想并使用 Java 实现的一套 MQ。
2. 2016年11月，阿里将 RocketMQ 捐献给 Apache 软件基金会，正式成为孵化项目。
3. 2017年2月20日，RocketMQ 正式发布 4.0 版本，新版本更适用于电商领域，金融领域，大数据领域，兼有物联网领域的编程模型。



#### RocketMQ基本概念


![](https://image-bed-20181207-1257458714.cos.ap-shanghai.myqcloud.com/back-end-2023/rocket-mq-structure-1.png)

![](https://image-bed-20181207-1257458714.cos.ap-shanghai.myqcloud.com/back-end-2022/mq-broker-info-1.jpeg)

RocketMQ 主要由 Producer、Broker、Consumer 三部分组成（还有一个部分是 NameServer），其中 Producer 负责生产消息，Consumer 负责消费消息，Broker 负责存储消息。

* 代理服务器（Broker Server）
消息中转角色，负责存储消息、转发消息。代理服务器在 RocketMQ 系统中负责接收从生产者发送来的消息并存储、同时为消费者的拉取请求作准备。代理服务器也存储消息相关的元数据，包括消费者组、消费进度偏移和主题和队列消息等。


![](https://image-bed-20181207-1257458714.cos.ap-shanghai.myqcloud.com/back-end-2023/rocket-mq-structure-2.png)

* 名字服务（Name Server）
  1. 名称服务充当路由消息的提供者（类似邮局的作用，如上图所示）。生产者或消费者能够通过名字服务（Name Server）查找各主题相应的 Broker IP 列表。多个 Name Server 实例组成集群，但相互独立，没有信息交换。
  2. NameServer 是一个无状态的服务器，角色类似于 Kafka 使用的 Zookeeper，但比 Zookeeper 更轻量。
  3. 每个 NameServer 结点之间是相互独立，彼此没有任何信息交互。
  4. Nameserver 被设计成几乎是无状态的，通过部署多个结点来标识自己是一个伪集群。
  5. Producer 在发送消息前从 NameServer 中获取 Topic 的路由信息也就是发往哪个 Broker。
  6. Consumer 也会定时从 NameServer 获取 Topic 的路由信息。
  7. Broker 在启动时会向 NameServer 注册，并定时进行心跳连接，且定时同步维护的 Topic 到 NameServer。


> Name Server 的角色类似 Dubbo中的 Zookeeper，但 NameServer 与 Zookeeper 相比更轻量，主要是因为每个 NameServer 节点互相之间是独立的，没有任何信息交互。NameServer 的主要开销是在维持心跳和提供 Topic-Broker 的关系数据。



* 拉取式消费（Pull Consumer）
Consumer 消费的一种类型，应用通常主动调用 Consumer 的拉消息方法从 Broker 服务器拉消息。主动权由应用控制，一旦获取了批量消息，应用就会启动消费过程。

* 推动式消费（Push Consumer）
Consumer 消费的一种类型，该模式下 Broker 收到数据后会主动推送给消费端，该消费模式一般实时性较高。



#### RocketMQ特性
* 订阅与发布
* 消息顺序：**RocketMQ 可以严格的保证消息有序**
* 消息过滤
* 消息可靠性
* 至少一次（At least Once）：每个消息必须投递一次。Consumer 先 Pull 消息到本地，消费完成后，才向服务器返回 Ack；如果没有消费一定不会 Ack 消息，所以 RocketMQ 可以很好的支持此特性。

> 消息领域有一个对消息投递的服务质量（Quality of Service，简称QoS），分为
> 1. 最多一次（At most once）
> 2. 至少一次（At least once）
> 3. 仅一次（ Exactly once）


* 回溯消费
* 事务消息
* 定时消息
* 消息重试：RocketMQ 对于重试消息的处理是先保存至 `Topic` 名称为 `SCHEDULE_TOPIC_XXXX` 的延迟队列中，后台定时任务按照对应的时间进行 Delay 后重新保存至 `%RETRY%+consumerGroup` 的重试队列中。
* 消息重投
* 流量控制
* 死信队列




#### RocketMQ的事务消息

本章节内容参见笔记「[架构-09-分布式事务](./../012-架构设计/架构-09-分布式事务.md#消息事务)」。


#### 死信队列

* [RocketMQ 管理死信队列](https://support.huaweicloud.com/usermanual-hrm/hrm-ug-023.html)



死信队列用于处理无法被正常消费的消息。

一条消息初次消费失败会被重试消费，若重试次数达到最大值（默认 16 次，在客户端可配置）时，依然消费失败，则其将被投递到该消费者对应的特殊队列（即死信队列）中，这种消息被称为「死信消息」。



分布式消息服务 RocketMQ 版提供三种死信消息查询的方法
1. 按 Group 查询
   * 查询某时间段内指定消费组下所有的死信消息。此方法属于范围查询，查询到的死信消息可能比较多
2. 按 Message ID 查询
   * 查询指定 Message ID 的消息，此方法属于精确查找，可以快速查询到某一条死信消息
3. 按 Message Key 查询
   * 查询指定 Message Key 的消息，此方法属于精确查找，可以快速查询到某一条死信消息



### 延时消息

* ref 1-[MQ 知识梳理 | 面经](https://heapdump.cn/article/3801373)


电商的订单超时自动取消，就是一个典型的利用延时消息的例子，用户提交了一个订单，就可以发送一个延时消息，1h 后去检查这个订单的状态，如果还是未付款就取消订单释放库存。


RocketMQ 是支持延时消息的，只需要在生产消息的时候设置消息的延时级别。

```java
// 实例化一个生产者来产生延时消息
DefaultMQProducer producer = new DefaultMQProducer("ExampleProducerGroup");
// 启动生产者
producer.start();
int totalMessagesToSend = 100;
for (int i = 0; i < totalMessagesToSend; i++) {
    Message message = new Message("TestTopic", ("Hello scheduled message " + i).getBytes());
    // 设置延时等级3,这个消息将在10s之后发送(现在只支持固定的几个时间,详看delayTimeLevel)
    message.setDelayTimeLevel(3);
    // 发送消息
    producer.send(message);
}
```

但是目前 RocketMQ 支持的延时级别是有限的。

```java
private String messageDelayLevel = "1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h";
```


**RocketMQ 具体是怎么实现延时消息的呢？简单八个字总结为「临时存储 + 定时任务」。**


Broker 收到延时消息了，会先发送到主题（SCHEDULE_TOPIC_XXXX）的相应时间段的 Message Queue 中，然后通过一个定时任务轮询这些队列。到期后，把消息投递到目标 Topic 的队列中，然后消费者就可以正常消费这些消息。

![](https://image-bed-20181207-1257458714.cos.ap-shanghai.myqcloud.com/back-end-2023/rocket-delay-mq-1.png)

## 引入MQ带来的问题

MQ 的三大应用是「异步处理」、「应用解耦」、「流量削峰」。在系统中引入MQ，会带来如下问题
1. 引入 MQ 中间件实现「应用解耦」，会影响系统之间数据传输的一致性，即消息生产端和消息消费端的消息数据一致性问题，所以要思考「如何保证消息不会丢失」。
2. 引入 MQ 中间件实现「流量削峰」，容易出现消费端处理能力不足从而导致消息积压，所以要思考「如何处理消息堆积」。
3. 发送的多条消息在达到消费端时，由于网络等原因，消息的消费的顺序，可消息发送的顺序会不一致，所以要思考「如何保证消息按顺序执行」。
4. 在消息消费的过程中，如果出现失败的情况，会通过补偿的机制执行重试。重试的过程就有可能产生重复的消息，所以要思考「如何保证消息不被重复消费」。


下面，将分别对这 4 个问题进行介绍。


### 如何保证消息不会丢失

要解决「如何保证消息不会丢失」问题，首先要知道哪些环节可能造成消息丢失。一个消息从产生到消费的过程如下图所示，共 3 个阶段。所以，消息丢失可能出现在
1. 消息生产阶段产生消息丢失
    * 网络传输中丢失消息
    * MQ发送异常
2. 消息存储阶段产生消息丢失
    * MQ 成功接收消息后，内部处理出错
    * Broker 宕机
3. 消息消费阶段产生消息丢失
    * 采用消息自动确认模式，消费者取到消息后未完成消费（或业务逻辑未执行完）

![](https://image-bed-20181207-1257458714.cos.ap-shanghai.myqcloud.com/back-end-2022/mq-handle-mq-lost-1.png)


理清楚了「哪些环节可能造成消息丢失」，就可对症下药，分析「如何保证消息不会丢失」。

#### 1.消息生产阶段保证消息不丢失

主流的 MQ 都有确认（Confirm）或事务机制，可以保证生产者将消息送达到 MQ。


> 方案1：采用事务机制

* 生产者在发送消息之前开启事物，然后发送消息。如果消息没有成功被 Broker 接收到，那么生产者会收到异常报错，此时生产者可以回滚事物，然后尝试重新发送；如果收到了消息，那么生产者就可以提交事物。
* 采用事务机制，在消息发送时，生产者会产生阻塞，等待是否发送成功，这会影响性能，造成吞吐量下降。

```java
  channel.txSelect();//开启事物
  try{
      //发送消息
  }catch(Exection e){
      channel.txRollback()；//回滚事物
      //重新提交
  }
```


> 方案2：采用确认（Confirm）

* 以 RabbitMQ 为例，生产者可以开启确认（Confirm）模式，每次写的消息都会分配一个唯一的 ID。Broker 在收到消息后，会返回一个 Ack 信号给生产者，确认消息发送成功。
* 事务机制是同步的，会造成阻塞。确认机制是异步的，生产者发送一条消息后可以接着发送下一个消息，不会产生阻塞。



```java
    //开启confirm
    channel.confirm();
    //发送成功回调
    public void ack(String messageId){
      
    }

    // 发送失败回调
    public void nack(String messageId){
        //重发该消息
    }
```


#### 2.消息存储阶段保证消息不丢失

* 开启 MQ 的持久化配置。
* 如果 Broker 是集群部署，有多副本机制，则消息不仅仅要写入当前 Broker，还需要写入副本机中，至少写入两台机子后，再给生产者返回确认 Ack 信号。

#### 3.消息消费阶段保证消息不丢失

改为手动确认模式，消费者成功消费消息后，再确认。




### 如何保证消息不被重复消费

解决该问题，有两个思路
1. 保证消息不会重复（实际不可行）
2. 保证消费重复消费不会产生影响（即保证消费端的幂等性）


为了保证消息不丢失，「失败重试」机制是必不可少的，所以消息被重复发送的现象，是无法避免的。既然消息一定会出现重复发送，因此，只能考虑「保证消费重复消费不会产生影响」，即「如何保证消费端的幂等性」。


关于「如何保实现幂等性」，参见「架构-02-幂等」。



### 如何保证消息按顺序执行

* ref 1-[如何保证消息的顺序性](https://xie.infoq.cn/article/c84491a814f99c7b9965732b1)
* ref 2-[消息队列经典十连问](https://mp.weixin.qq.com/s?__biz=Mzg3NzU5NTIwNg==&mid=2247497847&idx=1&sn=29a32672b712e7dfadfa36c9902b2ec7&chksm=cf22275ef855ae484fb3f51a5726e9a4bc45222e8fbbd33631d177dc4b5619c36889ea178463&token=1077989845&lang=zh_CN#rd)



保证消息按顺序执行，即保证「有序性」。「有序性」可分为
1. 全局有序
2. 部分有序

**全局顺序消息指某个 `Topic` 下的所有消息都要保证顺序。**

**部分顺序消息，只要保证每一组消息被顺序消费即可，比如订单消息，只要保证同一个订单 ID下的消息能按顺序消费即可。**

#### 1.全局有序

若要保证消息的全局有序，需要
1. 只能由一个生产者向 Topic 发送消息，并且一个 Topic 内部只能有一个队列（分区）
2. 消费者也必须是单线程消费这个队列


![](https://image-bed-20181207-1257458714.cos.ap-shanghai.myqcloud.com/back-end-2022/mq-handle-mq-all-sort-1.jpeg)



消息队列保证全局有序性的整体思路如上图所示。比如 Kafka 的全局有序消息，就是这种思想的体现，就是生产者发消息时，1 个 Topic 只能对应 1 个 Partition，一个 Consumer，内部单线程消费。

但是这样吞吐量太低，一般保证消息局部有序即可。在发消息的时候指定Partition Key，Kafka 对其进行 Hash 计算，根据计算结果决定放入哪个 Partition。这样 Partition Key 相同的消息会放在同一个Partition。然后多消费者单线程消费指定的 Partition。

一般情况下我们都不需要全局有序，保证局部有效即可。

#### 2.局部有序


若要保证消息的局部有序，需要
1. 将 Topic 内部划分成我们需要的队列数，把消息通过特定的策略发往固定的队列中
2. 每个队列对应一个单线程处理的消费者

这样即完成了部分有序的需求，又可以通过队列数量的并发来提高消息处理效率。

![](https://image-bed-20181207-1257458714.cos.ap-shanghai.myqcloud.com/back-end-2022/mq-handle-mq-part-sort-1.jpeg)


![](https://image-bed-20181207-1257458714.cos.ap-shanghai.myqcloud.com/back-end-2023/rocket-order-mq-1.png)

### 如何处理消息堆积


消息的堆积往往是因为生产者的生产速度与消费者的消费速度不匹配，原因可能是
1. 消息消费失败并反复重试，造成其余消息产生堆积
2. 消费者消费能力较弱，渐渐地产生消息堆积


因此，我们需要先定位消费慢的原因
* 如果是 bug 则处理 bug 
* 如果是因为本身消费能力较弱，我们可以优化下消费逻辑，比如之前是一条一条消息消费处理的，可以改为批量处理，如数据库的单条数据插入和批量插入
* 如果上面解决手段都无效，消费能力还是较弱，则**需要通过「水平扩容」来提升消费端的并发处理能力。增加 Topic 的队列数和消费者数量。注意队列数也要增加，不然新增加的消费者是没东西消费的。一个 Topic 中，一个队列只会分配给一个消费者。**

> **在扩容消费者的实例数的同时，必须同步扩容主题 Topic 的队列（分区）数量，确保消费者的实例数和分区数相等。如果消费者的实例数超过了分区数，由于分区是单线程消费，所以这样的扩容就没有效果。**
> 
> 比如在 Kafka 中，一个 Topic 可以配置多个 Partition（分区），数据会被写入到多个分区中。但在消费的时候，Kafka 约定一个分区只能被一个消费者消费，Topic 的分区数量决定了消费的能力，所以，可以通过增加分区来提高消费者的处理能力。


#### 如何临时扩容解决线上消息积压
* ref 1-[RabbitMQ消息队列常见面试题总结 | CSDN](https://blog.csdn.net/a745233700/article/details/115060109)

1. 如果是因为 consumer 故障，先修复 consumer 的问题，确保其恢复消费的能力。
2. 然后将现有的 consumer 都停掉。
3. 临时创建原先 N 倍数量的 queue ，然后写一个临时分发数据的消费者程序，将该程序部署上去消费队列中积压的数据，消费之后不做任何耗时处理，直接均匀轮询写入临时建立好的 N 倍数量的 queue 中。
4. 接着，临时征用 N 倍的机器来部署 consumer，每个 consumer 消费一个临时 queue 的数据。
5. 等快速消费完积压数据之后，恢复原先部署架构，重新用原先的 consumer 机器消费消息。

这种做法相当于临时将 queue 资源和 consumer 资源扩大 N 倍，以正常 N 倍速度消费。


## MQ 集群部署
* ref 1-[MQ 知识梳理 | 面经](https://heapdump.cn/article/3801373)
* ref 2-[RocketMQ企业级部署方案](https://www.moregeek.xyz/i/824000773102)
* ref 3-[MQ集群迁移过程中的双写+双读技术方案](https://wanchuan.top/f7eb44a71166433ab8b54de1f67abfcd)



### 如何保证RocketMQ的高可用

NameServer 因为是无状态，且不相互通信的，所以只要集群部署就可以保证高可用。

![](https://image-bed-20181207-1257458714.cos.ap-shanghai.myqcloud.com/back-end-2023/rocket-mq-multi-server-1.png)


**RocketMQ 的高可用主要是在体现在 Broker 的读和写的高可用，Broker 的高可用是通过集群和主从实现的。**

![](https://image-bed-20181207-1257458714.cos.ap-shanghai.myqcloud.com/back-end-2023/rocket-mq-multi-server-2.png)


Broker 可以配置两种角色
1. Master
2. Slave

Master 角色的 Broker 支持读和写，Slave 角色的 Broker 只支持读，Master 会向 Slave 同步消息。


也就是说 Producer 只能向 Master 角色的 Broker 写入消息，Cosumer 可以从 Master 和 Slave 角色的 Broker 读取消息。

Consumer 的配置文件中，并不需要设置是从 Master 读还是从 Slave读，当 Master 不可用或者繁忙的时候，Consumer 的读请求会被自动切换到从 Slave。有了自动切换 Consumer 这种机制，当一个 Master 角色的机器出现故障后，Consumer 仍然可以从 Slave 读取消息，不影响 Consumer 读取消息，这就实现了读的高可用。


如何达到发送端写的高可用性呢？
1. 在创建 Topic 的时候，把 Topic 的多个 Message Queue 创建在多个 Broker 组上（相同 Broker 名称，不同 brokerId 机器组成 Broker 组）
2. 这样当 Broker 组的 Master 不可用后，其他组 Master 仍然可用，Producer 仍然可以发送消息 RocketMQ。目前还不支持把Slave 自动转成 Master。
3. 如果机器资源不足，需要把 Slave 转成 Master ，则要手动停止 Slave 角色的 Broker，更改配置文件，用新的配置文件启动 Broker。



## 扩展阅读

* [京东JMQ4如何如何实现百万TPS](https://www.jiqizhixin.com/articles/2019-01-21-19)
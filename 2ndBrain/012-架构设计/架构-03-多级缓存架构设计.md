

# 架构-03-多级缓存架构设计


[TOC]


## 更新
* 2022/05/08，撰写




## 参考资料
* [Redis + Caffeine两级缓存 | 掘金](https://juejin.cn/post/7088195002580336653?share_token=0c5db138-b9f8-44f6-abd1-1c3089a84004)




## 多级缓存架构设计


在单纯使用「远程缓存或分布式缓存」（如 Redis 缓存）的一级缓存架构的基础上，可以再引入「本地缓存」，组成「两级缓存架构」，进一步提升程序的响应速度与服务性能，如下图所示。


![](https://image-bed-20181207-1257458714.cos.ap-shanghai.myqcloud.com/back-end-2020/multi-cache-design-1.png)




## 缓存的类型

就常见缓存类型而言，可以分为
1. 本地缓存
   * 存储在本地内存，又称 JVM 缓存
   * 常见的方案如 Google Guava Cache、Caffeine。Caffeine 性能优于 Guava Cache。
2. 分布式缓存（远程缓存）
   * 需要考虑数据一致性问题 
   * 常见的方案如 Redis



| 缓存的类型 |  描述  | 优点  | 缺点  | 常见的方案 |
|----------|--------|------|-------|----------|
| 本地缓存 | 存储在本地内存，又称 JVM 缓存 | 应用和缓存在同一个进程内部，请求缓存非常快，没有过多的网络开销 | 容量有限；每个 JVM 实例都会存一份，存在数据冗余 | Guava Cache、Caffeine |
| 分布式缓存 | 与应用分离的缓存组件或服务 | 存储容量大；和应用机器分开，不受应用影响；缓存全局共享 | 存在网络开销；相比本地缓存，存取速度较慢 | Redis |




## Google Guava Cache
* ref 1-[Guava Cache使用介绍 | 掘金](https://juejin.cn/post/6844903793629331470)


Guava Cache 是 Google 开源的 Java 重用工具集库 Guava 里的一款缓存工具，它的设计灵感来源于`ConcurrentHashMap`，使用多个 segments 方式的细粒度锁，在保证线程安全的同时，支持高并发场景需求，同时支持多种类型的缓存清理策略，包括基于容量的清理、基于时间的清理、基于引用的清理等。


### 加载

Guava Cache 有两种缓存加载的方式，这两种方式都是按照 “获取缓存-如果没有-则计算”（get-if-absent-compute）的规则加载的
1. `CacheLoader`
   * 在创建 Cache 的时候，实现了一个统一的根据 key 获取 value 的方法
2. `Callable`
   * 使用更加灵活，允许在 get 的时候指定一个 callable 来获取 value


### 淘汰策略

Guava Cache 中，数据是缓存在 Java 堆内存上的，存储容量受到堆内存大小限制。当缓存的数据量很大时，会影响到 GC，所以缓存必须要有回收策略。Guava Cache 提供了 3 种缓存淘汰策略
1. 基于容量回收
   * 当缓存容量达到最大值时，将使用 LRU 算法对缓存进行淘汰
2. 基于时间回收
   * 可以设置缓存在被写入或访问后，经过一定时间后被淘汰
3. 基于引用回收
   * `CacheBuilder.weakKeys()` 表示使用弱引用存储键，当键没有强引用或软引用时，缓存项可以被垃圾回收
   * `CacheBuilder.weakValues()` 表示使用弱引用存储值，当值没有强引用或软引用时，缓存项可以被垃圾回收
   * `CacheBuilder.softValues()` 表示使用软引用存储值。当有内存不足需要回收内存时，会按照 LRU 策略进行回收



### 显式清除
除了系统回收外，也可以主动清除缓存
* 个别清除：`Cache.invalidate(key)`
* 批量清除：`Cache.invalidateAll(keys)`
* 清除所有缓存项：`Cache.invalidateAll()`


## Caffeine
* ref 1-[Java高性能缓存库-Caffeine | Blog](https://ghh3809.github.io/2021/05/31/caffeine/)



### 什么是Caffeine


> Caffeine 借鉴了 Guava Cache 的设计思想，并进行了优化，性能比 Guava Cache 更强大。


`Caffeine` 是一个 Java 高性能的本地缓存库。`Caffeine` 采用了 `Window TinyLfu` 缓存淘汰算法，使得缓存命中率已经接近最优值。


`Caffeine` 的内部结果如下图所示，主要由 3 部分组成
1. `ConcurrentHashMap`：`ConcurrentHashMap` 是一个线程安全的容器，用于存放缓存数据。
2. `Scheduler`：定期清空数据的一个机制，可以不设置；如果不设置则不会主动的清空过期数据。
3. `Executor`：指定运行异步任务时要使用的线程池，可以不设置。如果不设置则会使用默认的线程池`ForkJoinPool.commonPool()`


> `Caffeine` 其实就是一个被强化过的 `ConcurrentHashMap`。这个「强化」指的是可以自动移除（淘汰）“不常用” 的数据。
> 
> 即`Caffeine` 是一种带有存储和移除策略的 `ConcurrentHashMap`。。


![](https://image-bed-20181207-1257458714.cos.ap-shanghai.myqcloud.com/back-end-2020/caffeine-structure-1.png)


### Caffeine的功能点

1. 自动加载条目到缓存中，支持异步加载
2. 异步刷新
4. **Key 会被包装成 Weak 引用**
5. **Value 会被包装成 Weak 或者 Soft 引用，从而能被 GC 掉，不至于内存泄漏**
6. 数据剔除提醒
7. 写入广播机制
8. 缓存访问可以统计
9. `Caffeine` 采用了 `Window TinyLfu` 缓存淘汰算法，使得缓存命中率已经接近最优值



### 缓存类型

Caffeine 共提供了 4 种类型的 Cache
1. `Cache`
   * 最普通的一种缓存，需要手动调用 `put()` 进行加载，手动调用 `invalidate()` 方法来移除缓存
   * 多线程下，调用 `get(key, k -> value)` 方法，可能会导致线程阻塞
2. `Loading Cache`
   * 是一种自动加载的缓存，当缓存不存在/已过期时，若调用 `get()` 方法，则会自动调用 `CacheLoader.load()` 方法加载最新值
3. `Async Cache`
   * 是 Cache 的一个变体，其响应结果均为 `CompletableFuture`，对异步编程模式进行了适配
   * 可以指定使用的线程池，若不指定，默认使用 `ForkJoinPool.commonPool()`
4. `Async Loading Cache`
   * 是 Loading Cache 和 Async Cache 的功能组合
   * 支持以异步的方式，对缓存进行自动加载。


### 淘汰策略



Caffeine 提供了多种淘汰策略（驱逐策略），常用的包括
1. 基于容量（大小）的驱逐
   * 当缓存容量达到最大值时，将使用 LRU 算法对缓存进行淘汰
2. 基于时间的驱逐
   * 可以设置缓存在被写入或访问后，经过一定时间后被淘汰
3. 基于引用回收
   * 同 Guava Cache 中的「基于引用回收」，此处不再赘述



驱逐策略可以组合使用，任意驱逐策略生效后，该缓存条目即被驱逐。

```java
// 创建一个最大容量为1000的缓存
Caffeine.newBuilder().maximumSize(1000).build(); 
// 创建一个写入10h后过期的缓存
Caffeine.newBuilder().expireAfterWrite(10, TimeUnit.HOURS).build(); 
// 创建一个访问1h后过期的缓存
Caffeine.newBuilder().expireAfterAccess(1, TimeUnit.HOURS).build(); 
```





### Window TinyLfu
* ref 1-[Caffeine Benchmark | Caffeine](https://github.com/ben-manes/caffeine/wiki/Benchmarks-zh-CN)
* ref 2-[Caffeine Window TinyLfu 算法分析 | Blog](https://www.cxyzjd.com/article/weixin_45727359/111714090)
* ref 3-[Caffeine 的 Window TinyLfu | CSDN](https://blog.csdn.net/weixin_40803011/article/details/117950738)


Caffeine 对 LFU 算法进行了改进，采用了 `Window TinyLfu` 的算法，使得缓存命中率已经接近最优值。

先来看一下 LFU 算法的两个缺点
1. 需要给每个记录项维护频率信息，每次访问都需要更新，这是个巨大的开销。
2. 无法缓存某一时间点的热点数据。对突发性的稀疏流量无力，因为前期经常访问的记录已经占用了缓存。

再看看一下 Window TinyLfu 是如何解决 LFU 算法的这两个缺点的
1. 针对第 1 个缺点，Window TinyLfu 采用了 `Count–Min Sketch` 算法（借鉴了布隆过滤器的设计思想）
2. 针对第 2 个缺点，Window TinyLfu 让记录尽量保持相对的 “新鲜”（Freshness Mechanism）
    * 增加一个窗口（window）来存储最新的数据，让其建立足够的频率，避免了稀疏流量问题
    * 当有新的记录插入时，可以让它跟老的记录进行 PK，输者就会被淘汰。这样一些老的、不再需要的记录就会被剔除


![](https://image-bed-20181207-1257458714.cos.ap-shanghai.myqcloud.com/back-end-2020/caffeine-window-tiny-lfu-1.png)




### 刷新策略

缓存过期后，下一次访问时会重新加载缓存，这会使线程处于阻塞状态。`Caffeine` 通过「刷新策略」可以避免这一问题。
* 使用刷新机制 `refreshAfterWrite()`，`Caffeine` 将在 `key` 允许刷新后的首次访问时，立即返回旧值，同时异步地对缓存值进行刷新，这使得调用方不至于因为缓存驱逐而被阻塞。
* 需要注意的是，刷新机制只支持 `LoadingCache` 和 `AsyncLoadingCache`。



### Spring Cache和Caffeine

  
Spring Cache 是 Spring 提供的一整套的缓存解决方案。Spring Cache 并没有提供缓存的实现，提供的是一整套的接口和代码规范、配置、注解等。


> Spring Framework 5.0（Spring Boot 2.0）版本后，将 `Caffeine` 代替 `Guava` 作为默认的缓存组件。


在 Spring 中使用 Caffeine 缓存，主要由两种方式
1. 手动实现 Spring Cache 提供的 `CaffeineCache` 接口和 `CacheManager` 接口
2. 使用 `@Cacheable`、`@CachePut`、`@CacheEvict`、`@CacheConfig` 相关注解进行开发。


> 关于 Spring Cache 的使用，参见「笔记-Spring-05-Spring缓存操作」。



## JdHotkey

* [JdHotkey | 京东毫秒级热key探测框架设计与实践，已实战于618大促](https://mp.weixin.qq.com/s/xOzEj5HtCeh_ezHDPHw6Jw)
* [JdHotkey | gitee](https://gitee.com/jd-platform-opensource/hotkey)




JdHotkey 是京东提供的一套通用轻量级热 Key 探测框架，可在 500ms 内探测出热 Key，并将其放入本地 JVM 缓存中，有较高的实时性，并有较高的性能，单台 8 核 8G 的机器在一秒内可检测 16W 个待测的 Key。



### 现有的热 Key 探测解决方案

对「Redis 热 key」问题的，除了增加 Redis 集群硬件配置外，还可以使用二级缓存（本地 JVM 缓存）进行解决。

针对二级缓存解决方案，热 Key 问题归根到底就是「如何找到热 Key，并将热 Key 放到 JVM 内存」的问题，有下面 3 种解决思路
1. 读取到 Redis 的 `key-value` 信息后，将热点 Key 缓存写入到本地 JVM  缓存中并设置合理的过期时间，或使用 Guava Cache、Caffeine Cache 缓存方案。这种方案无法主动对热点进行探测，当热点产生后，再去配置本地缓存，无法应对瞬时流量洪峰。
2. 改写 Redis 源码加入「热点探测」功能，有热 Key 时推送到 JVM。该方案实现起来有一定技术难度。
3. 改写 Jedis、Letture 等 Redis 客户端的 jar，通过本地计算来探测热点 Key，对热 Key 进行本地缓存并通知集群内其他机器进行缓存。



### JdHotkey的设计方案

![](https://image-bed-20181207-1257458714.cos.ap-shanghai.myqcloud.com/back-end-2022/jd-hot-key-1.png)

* `Etcd` 作为全局共用的配置中心，将让所有的 client 能读取到完全一致的 worker 信息和 rule 信息。
* `worker` 接收并计算各个 `client` 发来的 key，当某 key 达到规则里设定的阈值后，将其推送到该 APP 全部客户端 jar，之后推送到 etcd 一份，供 dashboard 监听记录。
* client 端启动后会连接 etcd，获取规则、获取专属的 worker ip 信息，之后持续监听该信息。
* client 会启动一个定时任务，每 500ms（可设置）就批量发送一次待测 key 到对应的 worker 机器，发送规则是 key 的 hashcode 对 worker 数量取余，所以固定的 key 肯定会发送到同一个 worker。这 500ms 内，就是本地搜集累加待测 key 及其数量，到期就批量发出去即可。
* **work 收到 client 发送的 key 信息后，对同一个 key 进行汇总计算，判断是否热 key。注意，热 key 的判断是在 work 端完成的，不是在 client 端，因为集群中有很多机器，一个 key 会在多个 client 端中使用，client 只是统计当前机器下 500ms 内 key 的调用次数。**
* 当 worker 探测出来热 key 后，会推送到 clinet 端，框架采用 caffeine 进行本地缓存，会根据当初设置的 rule 里的过期时间进行本地过期设置。
* JdHotkey中，关注的只有 key 本身，也就是一个字符串而已，而不关心 value，只对 key 进行探测。

### 高并发下SystemClock

JdHotkey 在热 Key 探测时，既没有修改 Redis 源码，也没有修改 Redis 客户端（如 Jedis 等）的 jar 包，而是引入一个独立的 jar 包，在该 jar 包中完成热 Key 探测，做到了最小侵入性。

在热 Key 探测时，在一定时间范围内计算 Key 的使用次数，若达到配置的阈值，则认为其为热 Key，将其对应的缓存放到本地 JVM 缓存中。


在「一定时间范围内计算 Key 的使用次数」的实现时，使用到了「滑动窗口」。

高并发下，使用 `System.currentTimeMillis()` 会有卡顿和性能问题，并发调用`System.currentTimeMillis()` 100次，耗费的时间是单线程调用 100 次的 250倍。`System.currentTimeMillis()` 是一个 `native` 方法，和具体系统版本、底层硬件结构有关。为解决 `System.currentTimeMillis()` 高并发下的卡顿问题，可以使用定时器去维护一个 `AtomicLong` 作为系统时钟，在内存中进行计数，保证高并发下的性能。详情可参考
* [高并发下滑动窗口的实现](https://blog.csdn.net/tianyaleixiaowu/article/details/102861254)
* [解决高并发下 System.currentTimeMillis 卡顿](https://www.programminghunter.com/article/44711579675/)


此处给出基于 `AtomicLong` 实现的 `SystemClock` 代码实现。

* SystemClock

```java
package com.baidu.utils;

import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicLong;

public class SystemClock {
    private final int period;

    private final AtomicLong now;

    private static class InstanceHolder {
        private static final SystemClock INSTANCE = new SystemClock(1);
    }
    //定时任务设置1毫秒
    private SystemClock(int period) {
        this.period = period;
        this.now = new AtomicLong(System.currentTimeMillis());
        scheduleClockUpdating();
    }

    private static SystemClock instance() {
        return InstanceHolder.INSTANCE;
    }

    private void scheduleClockUpdating() {
        //周期执行线程池
        ScheduledExecutorService scheduler = Executors.newSingleThreadScheduledExecutor(runnable -> {
            Thread thread = new Thread(runnable, "System Clock");
            //守护线程
            thread.setDaemon(true);
            return thread;
        });
        //任务，开始时间，间隔时间=周期执行，时间单位
        scheduler.scheduleAtFixedRate(() -> now.set(System.currentTimeMillis()), 0, period, TimeUnit.MILLISECONDS);
    }

    private long currentTimeMillis() {
        return now.get();
    }

    /**
     * 用来替换原来的System.currentTimeMillis()
     */
    public static long now() {
        return instance().currentTimeMillis();
    }
}
```

* 测试代码
  
```java
public class SystemClockTest {
    public static void main(String[] args) {

        int times=Integer.MAX_VALUE;
        System.out.println("times = " + times);

        long start = System.currentTimeMillis();
        for (long i = 0; i < times; i++) {
            SystemClock.now();
        }
        long end = System.currentTimeMillis();

        System.out.println("SystemClock Time:" + (end - start) + "毫秒");

        long start2 = System.currentTimeMillis();
        for (long i = 0; i < times; i++) {
            System.currentTimeMillis();
        }
        long end2 = System.currentTimeMillis();
        System.out.println("SystemCurrentTimeMillis Time:" + (end2 - start2) + "毫秒");
    }
}
```

* 运行测试方法，程序输出如下，在循环调用约 20 亿次时，`SystemClock` 的耗时只有 `System.currentTimeMillis()` 的约十分之一。


```s
times = 2147483647
SystemClock Time: 1102毫秒
SystemCurrentTimeMillis Time: 13304毫秒
```